## Overview

| Developed by | Guardrails AI |
| --- | --- |
| Date of development | Feb 15, 2024 |
| Validator type | Chatbots, QA |
| Blog |  |
| License | Apache 2 |
| Input/Output | Output |

## Description

This validator checks whether an answer is relevant to the question asked by asking the LLM to self evaluate.

### Intended use

The primary intended uses is for building chatbots, and verifying answer relevance for chatbots.

### Resources required

- Dependencies: Foundation model
- Foundation model access keys: Foundation model access keys

## Installation

```bash
$ gudardrails hub install hub://guardrails/qa_relevance_llm_eval
```

## Usage Examples

### Validating string output via Python

In this example, we apply the validator to a string output generated by an LLM.

```python
# Import Guard and Validator
from guardrails.hub import QARelevanceLLMEval
from guardrails import Guard

# Initialize Validator
val = QARelevanceLLMEval(
    llm_callable="openai",
    on_fail="noop"
)

# Create Guard with Validator
guard = Guard.from_string(validators=[val, ...])

guard(
    "LLM response",
    metadata={"question": "The question asked to the LLM"}
)
```

### Validating JSON output via Python

In this example, we apply the validator to a string field of a JSON output generated by an LLM.

```python
# Import Guard and Validator
from pydantic import BaseModel
from guardrails.hub import QARelevanceLLMEval
from guardrails import Guard

# Initialize Validator
val = QARelevanceLLMEval(
    llm_callable="openai",
    on_fail="noop"
)

# Create Pydantic BaseModel
class LLMQA(BaseModel):
    answer: str = Field(
        description="Answer generated by LLM", validators=[val]
    )

# Create a Guard to check for valid Pydantic output
guard = Guard.from_pydantic(output_class=LLMQA)

# Run LLM output generating JSON through guard
guard.parse("""
{
    "answer": "LLM generated answer",
}
""")
```

# API Reference

`__init__`

- `llm_callable`: The function to use for self evaluation. Use a Callable, or `openai`.
- `on_fail`: The policy to enact when a validator fails.
